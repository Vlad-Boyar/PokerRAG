import pandas as pd
import os
from dotenv import load_dotenv
from llama_index.core import VectorStoreIndex, Document, Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from telegram import Update
from telegram.ext import ApplicationBuilder, MessageHandler, ContextTypes, filters

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–ª—é—á–µ–π
load_dotenv()
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
TELEGRAM_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")

if not OPENAI_KEY or not TELEGRAM_TOKEN:
    raise ValueError("‚ùå –ù–µ —Ö–≤–∞—Ç–∞–µ—Ç –∫–ª—é—á–µ–π –≤ .env!")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LlamaIndex
Settings.llm = OpenAI(model="gpt-3.5-turbo", temperature=0)
Settings.embed_model = OpenAIEmbedding()

# –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∏–∑ CSV
df = pd.read_csv("faq.csv")
documents = [
    Document(text=row["answer"], metadata={"question": row["question"]})
    for _, row in df.iterrows()
]
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if update.message:
        print(f"[DEBUG] –ü—Ä–∏—à–ª–æ —Å–æ–æ–±—â–µ–Ω–∏–µ: {update.message.text}")
        await update.message.reply_text("–û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω")
    else:
        print("[DEBUG] –ü—Ä–∏—à–ª–æ —á—Ç–æ-—Ç–æ, –Ω–æ –Ω–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ")

# --- –ó–ê–ü–£–°–ö –ù–ê WINDOWS ---
import asyncio

app = ApplicationBuilder().token(TELEGRAM_TOKEN).build()
app.add_handler(MessageHandler(filters.TEXT, handle_message))

print("ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω. –û–∂–∏–¥–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –≤ Telegram...")
curl https://api.telegram.org/bot<your_token>/getMe
if os.name == "nt":
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    loop.run_until_complete(app.initialize())
    loop.create_task(app.start())
    loop.run_forever()
else:
    asyncio.run(app.run_polling())
