import os
import pandas as pd
from dotenv import load_dotenv
from telegram import Update
from telegram.ext import ApplicationBuilder, ContextTypes, CommandHandler, MessageHandler, filters

from llama_index.core import VectorStoreIndex, Settings, Document
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

import asyncio

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤
load_dotenv()
TELEGRAM_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LLM –∏ —ç–º–±–µ–¥–¥–µ—Ä–æ–≤
Settings.llm = OpenAI(model="gpt-3.5-turbo", api_key=OPENAI_API_KEY)

Settings.embed_model = HuggingFaceEmbedding(
    model_name="intfloat/multilingual-e5-base",
    embed_batch_size=8,
    query_instruction="query:",
    text_instruction="passage:"
)

# –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã
def load_documents_from_csv(csv_path="faq.csv"):
    df = pd.read_csv(csv_path)
    return [Document(text=f"Q: {row['question']}\nA: {row['answer']}") for _, row in df.iterrows()]

documents = load_documents_from_csv()
index = VectorStoreIndex.from_documents(documents)
retriever = index.as_retriever(similarity_top_k=3)

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
async def handle(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.message.text
    print(f"[RAG] –í–æ–ø—Ä–æ—Å –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}")

    nodes = retriever.retrieve(query)
    if not nodes:
        await update.message.reply_text("‚ùå –ù–µ –Ω–∞—à—ë–ª –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –æ—Ç–≤–µ—Ç–∞.")
        return

    top_node = nodes[0]
    top_score = top_node.score or 0.0

    # –ü–æ—Ä–æ–≥ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏
    if top_score < 0.5:
        await update.message.reply_text("‚ùå –Ø –Ω–µ —É–≤–µ—Ä–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ ‚Äî –≤ –±–∞–∑–µ –Ω–µ—Ç –±–ª–∏–∑–∫–∏—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π.")
        return

    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
    context_str = top_node.get_content()
    prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context_str}\n\n–í–æ–ø—Ä–æ—Å: {query}\n–û—Ç–≤–µ—Ç:"
    response = Settings.llm.complete(prompt).text.strip()

    # –í—ã–≤–æ–¥ —Ç–æ–ø-3 –≤–æ–ø—Ä–æ—Å–æ–≤ —Å score
    explanation = "üîç –ü–æ—Ö–æ–∂–∏–µ –≤–æ–ø—Ä–æ—Å—ã:\n"
    for i, node in enumerate(nodes[:3]):
        score = node.score or 0.0
        question_line = node.get_content().split("\n")[0].replace("Q: ", "").strip()
        explanation += f"{i+1}. {question_line}  ‚Äî  üìä {score:.2f}\n"

    final_message = f"{response}\n\n{explanation}"
    await update.message.reply_text(final_message)

# –ö–æ–º–∞–Ω–¥–∞ /start
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("–ü—Ä–∏–≤–µ—Ç! –°–ø—Ä–æ—Å–∏ —á—Ç–æ-–Ω–∏–±—É–¥—å ‚Äî –∏ —è –ø–æ—Å—Ç–∞—Ä–∞—é—Å—å –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.")

# –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
async def main():
    app = ApplicationBuilder().token(TELEGRAM_TOKEN).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle))

    print("ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω –∏ –∂–¥—ë—Ç –≤–æ–ø—Ä–æ—Å–æ–≤.")
    await app.initialize()
    await app.start()
    await app.updater.start_polling()
    while True:
        await asyncio.sleep(1)

if __name__ == "__main__":
    asyncio.run(main())
